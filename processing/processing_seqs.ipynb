{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b5f341-16b3-4b1c-9292-7721eebdb27c",
   "metadata": {},
   "source": [
    "# Processing Raw Sequences\n",
    "\n",
    "Scripts used to trim raw sequences, check quality, map to ref."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bc72c-11d2-43a8-90cf-522ea83a5d1c",
   "metadata": {},
   "source": [
    "## 1. trimming \n",
    "\n",
    "using `trim-galore` - [documentation](https://github.com/FelixKrueger/TrimGalore)\n",
    "\n",
    "This code uses an array to run jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e88c12-4114-443f-9ecf-9ef7e68eb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=trim_galore_array\n",
    "#SBATCH -c 4\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --array=1-120\n",
    "#SBATCH -o slurm-%A_%a.out\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "module load conda/latest\n",
    "\n",
    "conda activate cutadapt\n",
    "# trim-galore is already installed in this env \n",
    "\n",
    "#---------------change wd----------------#\n",
    "\n",
    "# to scratch workspace with downloaded seqs\n",
    "\n",
    "cd /scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "# parent directory containing sample subdirectories\n",
    "PARENT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/01.RawData\"\n",
    "\n",
    "# output dir for all trimmed files\n",
    "OUTDIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "\n",
    "SAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" /scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/sample_dirs.txt)\n",
    "\n",
    "R1=(\"$SAMPLE\"/*_1*.fq.gz)\n",
    "R2=(\"$SAMPLE\"/*_2*.fq.gz)\n",
    "\n",
    "echo \"Running Trim Galore on: $SAMPLE\"\n",
    "\n",
    "trim_galore --paired --fastqc -j 4 -o \"$OUTDIR\" \"${R1[@]}\" \"${R2[@]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c9930-0398-486b-8e67-c769189a2ab0",
   "metadata": {},
   "source": [
    "## 2. check quality\n",
    "using FastQC and MultiQC to check quality after trimming adapters\n",
    "\n",
    "**2a. FastQC** to generate quality assessment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4995f4-caa4-402c-9681-a9beebafcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=fastqc_array\n",
    "#SBATCH -c 4                 # cores per task\n",
    "#SBATCH --mem=16G             # memory per node\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --array=1-120         # number of array tasks\n",
    "#SBATCH -o slurm-%A_%a.out\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# Load conda and activate environment\n",
    "module load conda/latest\n",
    "conda activate fastqc\n",
    "\n",
    "# Set working directories\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/fastqc\"\n",
    "cd \"$INPUT_DIR\"\n",
    "\n",
    "# Number of files per task\n",
    "FILES_PER_TASK=4\n",
    "\n",
    "# Compute which lines (files) this array task will process\n",
    "START=$(( (SLURM_ARRAY_TASK_ID - 1) * FILES_PER_TASK + 1 ))\n",
    "END=$(( SLURM_ARRAY_TASK_ID * FILES_PER_TASK ))\n",
    "\n",
    "# Loop over assigned files\n",
    "for i in $(seq $START $END); do\n",
    "    FILE=$(sed -n \"${i}p\" fq_files.txt)\n",
    "    if [ -n \"$FILE\" ]; then  # skip if line is empty\n",
    "        echo \"Processing $FILE\"\n",
    "        fastqc -t 2 -o \"$OUTPUT_DIR\" \"$FILE\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e7c29-b395-4681-9c58-b3d4c9bd0bc1",
   "metadata": {},
   "source": [
    "**2b. MultiQC** to view all 120 samples (480 files) at once\n",
    "\n",
    ">multiqc runs quickly, so can be done in terminal and don't need to submit a job. both the html and zip files need to be in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1beee17-0ab3-4956-9f57-2b13f5966072",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiqc ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e62ca2-8b9d-4de4-bd59-8e5f09ac8e7d",
   "metadata": {},
   "source": [
    "## 3. alignment\n",
    "\n",
    "using `hisat2` ([manual](https://daehwankimlab.github.io/hisat2/manual/)) - following pipeline from [how to page](https://daehwankimlab.github.io/hisat2/howto/)\n",
    "\n",
    "(using hisat2 over bowtie2 bc bowtie2 isn't splice aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41778d-c831-44f3-ac1b-e1c44d9e5a8b",
   "metadata": {},
   "source": [
    "#### 3a. build genome index with exons and splice sites\n",
    "download reference genome (genome.fa) and GTF file (to make exon, splice site file; genome.gtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a987f6-253b-4333-9301-7057dfa633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/022/765/GCF_002022765.2_C_virginica-3.0/GCF_002022765.2_C_virginica-3.0_genomic.gff.gz\n",
    "gunzip GCF_002022765.2_C_virginica-3.0_genomic.gff.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50f221-6f09-46cd-8a67-e93b06246e28",
   "metadata": {},
   "source": [
    "make exon and splice sites files from GTF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df506f01-16a8-4ee3-baf2-50e45e0d14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract splice sites\n",
    "hisat2_extract_splice_sites.py GCF_002022765.2_C_virginica-3.0_genomic.gff > oyster.ss\n",
    "\n",
    "# Extract exons\n",
    "hisat2_extract_exons.py GCF_002022765.2_C_virginica-3.0_genomic.gff > oyster.exon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7e6d-e4bf-4c7b-8ba5-c52e6cadc267",
   "metadata": {},
   "source": [
    "Build HFM index - with exon and splice site info using the files above\n",
    "\n",
    "(HFM = hierarchical FM index for a reference genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca6a8b-cde5-4c87-ac76-ca91a9f7676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=hisat2_build\n",
    "#SBATCH -c 8                 # cores per task\n",
    "#SBATCH --mem=64G             # memory per node\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 1:00:00\n",
    "#SBATCH --cpus-per-task=16     \n",
    "#SBATCH -o hisat2_build.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "module load conda/latest\n",
    "\n",
    "conda activate hisat2-env\n",
    "\n",
    "#---------------change wd----------------#\n",
    "\n",
    "cd /work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "hisat2-build -p 16 \\\n",
    "--exon oyster.exon \\\n",
    "--ss oyster.ss \\\n",
    "GCF_002022765.2_C_virginica-3.0_genomic.fna oyster_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5288b5-5d0a-4fda-b6ea-ca2054371721",
   "metadata": {},
   "source": [
    "#### 3b. align sequences to genome index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae880197-6fd8-4025-af6e-a4c7d3d1206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=hisat2_align\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 32:00:00\n",
    "#SBATCH -o hisat2_align_%j.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "\n",
    "module load conda/latest\n",
    "\n",
    "conda activate hisat2-env\n",
    "\n",
    "#-----------------set paths----------------#\n",
    "\n",
    "INDEX=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files/hisat2_index\"\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/hisat2-align\"\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "for R1 in ${INPUT_DIR}/*_gi_1_val_1.fq.gz; do\n",
    "    # Remove _gi_1_val_1.fq.gz to get sample base name\n",
    "    SAMPLE=$(basename $R1 _gi_1_val_1.fq.gz)\n",
    "    \n",
    "    # Construct R2 with gi_2\n",
    "    R2=\"${INPUT_DIR}/${SAMPLE}_gi_2_val_2.fq.gz\"\n",
    "    \n",
    "    # Check if R2 exists\n",
    "    if [[ ! -f \"$R2\" ]]; then\n",
    "        echo \"Warning: R2 file not found: $R2\"\n",
    "        continue\n",
    "    fi\n",
    "    \n",
    "    echo \"Aligning $SAMPLE...\"\n",
    "    echo \"R1: $R1\"\n",
    "    echo \"R2: $R2\"\n",
    "    \n",
    "    hisat2 -p 16 \\\n",
    "        -x $INDEX/oyster_index \\\n",
    "        -1 $R1 \\\n",
    "        -2 $R2 \\\n",
    "        -S ${OUTPUT_DIR}/${SAMPLE}.sam \\\n",
    "        2> ${OUTPUT_DIR}/${SAMPLE}.log\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ce6f0-3376-4e73-a563-ea5836d788e3",
   "metadata": {},
   "source": [
    "the above code takes ~32 hours to finish - below is an (untested!) code for running multiple alignments in parallel with an array job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac35df-4f80-48f9-81ce-c9ec522ad079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=hisat2_align\n",
    "#SBATCH --array=1-120%10\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 6:00:00\n",
    "#SBATCH -o logs/hisat2_align_%A_%a.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "module load conda/latest\n",
    "conda activate hisat2-env\n",
    "\n",
    "#-----------------set paths----------------#\n",
    "INDEX=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files/hisat2_index\"\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/hisat2-align\"\n",
    "\n",
    "# Get list of R1 files\n",
    "R1_FILES=(\"${INPUT_DIR}\"/*_gi_1_val_1.fq.gz)\n",
    "\n",
    "# Get the R1 file for this array task\n",
    "R1=\"${R1_FILES[$SLURM_ARRAY_TASK_ID-1]}\"\n",
    "\n",
    "# Get sample base name\n",
    "SAMPLE=$(basename \"$R1\" _gi_1_val_1.fq.gz)\n",
    "\n",
    "# Construct R2 filename\n",
    "R2=\"${INPUT_DIR}/${SAMPLE}_gi_2_val_2.fq.gz\"\n",
    "\n",
    "# Check if R2 exists\n",
    "if [[ ! -f \"$R2\" ]]; then\n",
    "    echo \"Error: R2 file not found: $R2\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#-----------------commands----------------#\n",
    "echo \"Aligning $SAMPLE...\"\n",
    "echo \"R1: $R1\"\n",
    "echo \"R2: $R2\"\n",
    "\n",
    "hisat2 -p 16 \\\n",
    "    -x \"$INDEX/oyster_index\" \\\n",
    "    -1 \"$R1\" \\\n",
    "    -2 \"$R2\" \\\n",
    "    -S \"${OUTPUT_DIR}/${SAMPLE}.sam\" \\\n",
    "    2> \"${OUTPUT_DIR}/${SAMPLE}.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e97c7-3e33-4508-a0e0-1366f83664e7",
   "metadata": {},
   "source": [
    "## 4. convert SAM to BAM\n",
    "using `samtools` ([documentation](https://www.htslib.org/doc/samtools-view.html)) to convert SAM files to BAM and sort by genomic coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7c028-bfdb-479f-b8b8-93969533838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=sam2bam\n",
    "#SBATCH --array=1-120\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 4:00:00\n",
    "#SBATCH -o logs/sam2bam_%A_%a.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "#-----------------modules-----------------#\n",
    "\n",
    "module load samtools/1.19.2\n",
    "\n",
    "#-----------------set paths----------------#\n",
    "\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/hisat2-align/\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/bam_files/\"\n",
    "\n",
    "# Get list of SAM files\n",
    "SAMPLES=(\"$INPUT_DIR\"*.sam)\n",
    "SAM_FILE=\"${SAMPLES[$SLURM_ARRAY_TASK_ID-1]}\"\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "# Convert and sort (by genomic coordinate)\n",
    "BASE=$(basename \"$SAM_FILE\" .sam)\n",
    "samtools view -@ 4 -bS \"$SAM_FILE\" | samtools sort -@ 4 -o \"${OUTPUT_DIR}${BASE}.sorted.bam\"\n",
    "samtools index \"${OUTPUT_DIR}${BASE}.sorted.bam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96abb22-f9c7-48f8-8a9c-ee377519088c",
   "metadata": {},
   "source": [
    "## 5. read counting\n",
    "with `Subread-featureCounts' ([manual](https://subread.sourceforge.net/featureCounts.html)) to generate counts matrix\n",
    "\n",
    "\n",
    "featureCounts recommends to use the same GTF/GFF file that was used for alignment - so using GFF file, but removing the first couple of lines that start with \"#\" so that featureCounts can read it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b764b5-800f-4eb8-8b45-681f906e3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove #s in first couple of lines of gff file\n",
    "grep -v '^#' GCF_002022765.2_C_virginica-3.0_genomic.gff > cv.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d7444-dec0-42dd-abf2-721535cbb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=featureCounts\n",
    "#SBATCH --array=1-120          \n",
    "#SBATCH --cpus-per-task=8      # more threads for featureCounts\n",
    "#SBATCH --mem=16G               \n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 1:00:00              \n",
    "#SBATCH --output=logs/featureCounts_%A_%a.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "#-----------------modules-----------------#\n",
    "\n",
    "module load conda/latest\n",
    "conda activate subread_env\n",
    "\n",
    "#-----------------set paths----------------#\n",
    "\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/bam_files/\"\n",
    "OUTPUT_DIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ce24_rnaseq\"\n",
    "\n",
    "BAM_LIST=\"$INPUT_DIR/bam_list.txt\"\n",
    "\n",
    "GFF_FILE=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files/genome/cv.gff\"\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "# Get the BAM file for this array task\n",
    "BAM_FILE=\"$INPUT_DIR/$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" \"$BAM_LIST\")\"\n",
    "\n",
    "SAMPLE_NAME=$(basename \"$BAM_FILE\" .sorted.bam)\n",
    "\n",
    "# Run featureCounts\n",
    "featureCounts \\\n",
    "-T 8 \\\n",
    "-a \"$GFF_FILE\" \\\n",
    "-g ID \\\n",
    "-t gene \\\n",
    "-p \\\n",
    "-B \\\n",
    "-C \\\n",
    "-s 0 \\\n",
    "-o \"$OUTPUT_DIR/counts_${SAMPLE_NAME}.txt\" \\\n",
    "\"$BAM_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14efe9-b3ff-4e51-a46b-b284c0a38905",
   "metadata": {},
   "source": [
    "annotation of featureCounts options:\n",
    "```\n",
    "featureCounts \\\n",
    "  -T 8 \\                  # threads\n",
    "  -a \"$gff.file\" \\        # GTF annotation\n",
    "  -g ID \\                 # gene identifier attribute\n",
    "  -t gene \\               # feature type\n",
    "  -p \\                    # paired-end\n",
    "  -B \\                    # require both ends mapped\n",
    "  -C \\                    # ignore chimeric fragments\n",
    "  -s 0 \\                  # unstranded\n",
    "  -o \"$OUTPUT_DIR/counts_${SLURM_ARRAY_TASK_ID}.txt\" \\\n",
    "  \"$BAM_FILE\"\n",
    "```\n",
    "\n",
    "the output of this will be multiple .txt files, two for each sample (one containing the counts matrix, the other containing summaries about read counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2dcbcc-4640-451d-b067-b7f882554d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "INPUT_DIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ce24_rnaseq/featureCounts/\"\n",
    "OUTPUT_FILE=\"//project/pi_sarah_gignouxwolfsohn_uml_edu/julia/CE_2024/CE24_RNA-seq/processing/qc_outputs/featureCounts_summary.csv\"\n",
    "\n",
    "# Get header from the first file\n",
    "first_file=$(ls $INPUT_DIR/*.summary | head -n 1)\n",
    "awk 'NR>1 {print $1}' \"$first_file\" | paste -sd ',' - > \"$OUTPUT_FILE.header\"\n",
    "\n",
    "# Start the CSV with \"Sample\" column\n",
    "echo \"Sample,$(cat $OUTPUT_FILE.header)\" > \"$OUTPUT_FILE\"\n",
    "\n",
    "# Loop over all files\n",
    "for f in $INPUT_DIR/*.summary; do\n",
    "    sample=$(basename \"$f\" .txt.summary)  # extract sample name\n",
    "    awk 'NR>1 {print $2}' \"$f\" | paste -sd ',' - | awk -v s=\"$sample\" '{print s \",\" $0}' >> \"$OUTPUT_FILE\"\n",
    "done\n",
    "\n",
    "# Clean up\n",
    "rm \"$OUTPUT_FILE.header\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:.conda-r-env]",
   "language": "R",
   "name": "conda-env-.conda-r-env-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
