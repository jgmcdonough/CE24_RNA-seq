{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b5f341-16b3-4b1c-9292-7721eebdb27c",
   "metadata": {},
   "source": [
    "# Processing Raw Sequences\n",
    "\n",
    "Scripts used to trim raw sequences, check quality, map to ref."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bc72c-11d2-43a8-90cf-522ea83a5d1c",
   "metadata": {},
   "source": [
    "## 1. trimming \n",
    "\n",
    "using `trim-galore` - [documentation](https://github.com/FelixKrueger/TrimGalore)\n",
    "\n",
    "This code uses an array to run jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e88c12-4114-443f-9ecf-9ef7e68eb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=trim_galore_array\n",
    "#SBATCH -c 4\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --array=1-120\n",
    "#SBATCH -o slurm-%A_%a.out\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "module load conda/latest\n",
    "\n",
    "conda activate cutadapt\n",
    "# trim-galore is already installed in this env \n",
    "\n",
    "#---------------change wd----------------#\n",
    "\n",
    "# to scratch workspace with downloaded seqs\n",
    "\n",
    "cd /scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "# parent directory containing sample subdirectories\n",
    "PARENT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/01.RawData\"\n",
    "\n",
    "# output dir for all trimmed files\n",
    "OUTDIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "\n",
    "SAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" /scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/sample_dirs.txt)\n",
    "\n",
    "R1=(\"$SAMPLE\"/*_1*.fq.gz)\n",
    "R2=(\"$SAMPLE\"/*_2*.fq.gz)\n",
    "\n",
    "echo \"Running Trim Galore on: $SAMPLE\"\n",
    "\n",
    "trim_galore --paired --fastqc -j 4 -o \"$OUTDIR\" \"${R1[@]}\" \"${R2[@]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c9930-0398-486b-8e67-c769189a2ab0",
   "metadata": {},
   "source": [
    "## 2. check quality\n",
    "using FastQC and MultiQC to check quality after trimming adapters\n",
    "\n",
    "**2a. FastQC** to generate quality assessment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4995f4-caa4-402c-9681-a9beebafcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=fastqc_array\n",
    "#SBATCH -c 4                 # cores per task\n",
    "#SBATCH --mem=16G             # memory per node\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --array=1-120         # number of array tasks\n",
    "#SBATCH -o slurm-%A_%a.out\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# Load conda and activate environment\n",
    "module load conda/latest\n",
    "conda activate fastqc\n",
    "\n",
    "# Set working directories\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/fastqc\"\n",
    "cd \"$INPUT_DIR\"\n",
    "\n",
    "# Number of files per task\n",
    "FILES_PER_TASK=4\n",
    "\n",
    "# Compute which lines (files) this array task will process\n",
    "START=$(( (SLURM_ARRAY_TASK_ID - 1) * FILES_PER_TASK + 1 ))\n",
    "END=$(( SLURM_ARRAY_TASK_ID * FILES_PER_TASK ))\n",
    "\n",
    "# Loop over assigned files\n",
    "for i in $(seq $START $END); do\n",
    "    FILE=$(sed -n \"${i}p\" fq_files.txt)\n",
    "    if [ -n \"$FILE\" ]; then  # skip if line is empty\n",
    "        echo \"Processing $FILE\"\n",
    "        fastqc -t 2 -o \"$OUTPUT_DIR\" \"$FILE\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e7c29-b395-4681-9c58-b3d4c9bd0bc1",
   "metadata": {},
   "source": [
    "**2b. MultiQC** to view all 120 samples (480 files) at once\n",
    "\n",
    ">multiqc runs quickly, so can be done in terminal and don't need to submit a job. both the html and zip files need to be in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1beee17-0ab3-4956-9f57-2b13f5966072",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiqc ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e62ca2-8b9d-4de4-bd59-8e5f09ac8e7d",
   "metadata": {},
   "source": [
    "## 3. alignment\n",
    "\n",
    "using `hisat2` ([manual](https://daehwankimlab.github.io/hisat2/manual/)) - following pipeline from [how to page](https://daehwankimlab.github.io/hisat2/howto/)\n",
    "\n",
    "(using hisat2 over bowtie2 bc bowtie2 isn't splice aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41778d-c831-44f3-ac1b-e1c44d9e5a8b",
   "metadata": {},
   "source": [
    "#### 3a. build genome index with exons and splice sites\n",
    "download reference genome (genome.fa) and GTF file (to make exon, splice site file; genome.gtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a987f6-253b-4333-9301-7057dfa633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/022/765/GCF_002022765.2_C_virginica-3.0/GCF_002022765.2_C_virginica-3.0_genomic.gff.gz\n",
    "gunzip GCF_002022765.2_C_virginica-3.0_genomic.gff.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50f221-6f09-46cd-8a67-e93b06246e28",
   "metadata": {},
   "source": [
    "make exon and splice sites files from GTF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df506f01-16a8-4ee3-baf2-50e45e0d14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract splice sites\n",
    "hisat2_extract_splice_sites.py GCF_002022765.2_C_virginica-3.0_genomic.gff > oyster.ss\n",
    "\n",
    "# Extract exons\n",
    "hisat2_extract_exons.py GCF_002022765.2_C_virginica-3.0_genomic.gff > oyster.exon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7e6d-e4bf-4c7b-8ba5-c52e6cadc267",
   "metadata": {},
   "source": [
    "Build HFM index - with exon and splice site info using the files above\n",
    "\n",
    "(HFM = hierarchical FM index for a reference genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca6a8b-cde5-4c87-ac76-ca91a9f7676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=hisat2_build\n",
    "#SBATCH -c 8                 # cores per task\n",
    "#SBATCH --mem=64G             # memory per node\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 1:00:00\n",
    "#SBATCH --cpus-per-task=16     \n",
    "#SBATCH -o hisat2_build.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "module load conda/latest\n",
    "\n",
    "conda activate hisat2-env\n",
    "\n",
    "#---------------change wd----------------#\n",
    "\n",
    "cd /work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "hisat2-build -p 16 \\\n",
    "--exon oyster.exon \\\n",
    "--ss oyster.ss \\\n",
    "GCF_002022765.2_C_virginica-3.0_genomic.fna oyster_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5288b5-5d0a-4fda-b6ea-ca2054371721",
   "metadata": {},
   "source": [
    "#### 3b. align sequences to genome index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae880197-6fd8-4025-af6e-a4c7d3d1206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=hisat2_align\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -p cpu\n",
    "#SBATCH -t 4:00:00\n",
    "#SBATCH -o hisat2_align_%j.log\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "#-----------------modules-----------------#\n",
    "\n",
    "module load conda/latest\n",
    "\n",
    "conda activate hisat2-env\n",
    "\n",
    "#-----------------set paths----------------#\n",
    "\n",
    "INDEX=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/julia_mcdonough_student_uml_edu/ref_files/hisat2_index\"\n",
    "INPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/trimmed_all\"\n",
    "OUTPUT_DIR=\"/scratch4/workspace/julia_mcdonough_student_uml_edu-novogene_dwnld/hisat2-align\"\n",
    "\n",
    "#-----------------commands----------------#\n",
    "\n",
    "for R1 in ${INPUT_DIR}/*_gi_1_val_1.fq.gz; do\n",
    "    # Remove _gi_1_val_1.fq.gz to get sample base name\n",
    "    SAMPLE=$(basename $R1 _gi_1_val_1.fq.gz)\n",
    "    \n",
    "    # Construct R2 with gi_2\n",
    "    R2=\"${INPUT_DIR}/${SAMPLE}_gi_2_val_2.fq.gz\"\n",
    "    \n",
    "    # Check if R2 exists\n",
    "    if [[ ! -f \"$R2\" ]]; then\n",
    "        echo \"Warning: R2 file not found: $R2\"\n",
    "        continue\n",
    "    fi\n",
    "    \n",
    "    echo \"Aligning $SAMPLE...\"\n",
    "    echo \"R1: $R1\"\n",
    "    echo \"R2: $R2\"\n",
    "    \n",
    "    hisat2 -p 16 \\\n",
    "        -x $INDEX/oyster_index \\\n",
    "        -1 $R1 \\\n",
    "        -2 $R2 \\\n",
    "        -S ${OUTPUT_DIR}/${SAMPLE}.sam \\\n",
    "        2> ${OUTPUT_DIR}/${SAMPLE}.log\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:.conda-r-env]",
   "language": "R",
   "name": "conda-env-.conda-r-env-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
